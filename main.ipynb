{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KJanzon/project-nlp-challenge/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Fake News"
      ],
      "metadata": {
        "id": "C9wJV7Krl8Wy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task:\n",
        "- Train the model on training data set.\n",
        "- Validate model on training data set.\n",
        "- Predict real/fake news on 'testing_data' data set.\n",
        "-- label: 0 if the news is fake, 1 if the news is real.\n",
        "\n",
        "\n",
        "Steps:\n",
        "\n",
        "- Load data from google drive\n",
        "- Data pre-processing\n",
        "-- data cleaning\n",
        "-- stop words removal\n",
        "\n",
        "- Train/Validation split (on training data set)\n",
        "- Plot validation result\n",
        "-- training/validation loss\n",
        "-- training/validation accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tw2UCskjmUTO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "V72uNVnLl4jg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQbEuvFltjqn",
        "outputId": "a6d748aa-1629-4d2e-89f8-a55c5a77a552"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Define paths to the CSV files\n",
        "training_data_path = \"/content/drive/My Drive/training_data_lowercase.csv\"\n",
        "testing_data_path = \"/content/drive/My Drive/testing_data_lowercase_nolabels.csv\"\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(training_data_path, sep='\\t', header=None, names=['label', 'text'])\n",
        "test_df = pd.read_csv(testing_data_path, sep='\\t', header=None, names=['label','text'])\n",
        "\n",
        "# Display first few rows\n",
        "print(\"Training Data Sample:\")\n",
        "print(train_df.head())\n",
        "\n",
        "print(\"\\nTesting Data Sample:\")\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cE7OBr1mCOL",
        "outputId": "2dca613e-3372-41bf-e27e-adfe31cd67c3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Training Data Sample:\n",
            "   label                                               text\n",
            "0      0  donald trump sends out embarrassing new year‚s...\n",
            "1      0  drunk bragging trump staffer started russian c...\n",
            "2      0  sheriff david clarke becomes an internet joke ...\n",
            "3      0  trump is so obsessed he even has obama‚s name ...\n",
            "4      0  pope francis just called out donald trump duri...\n",
            "\n",
            "Testing Data Sample:\n",
            "  label                                               text\n",
            "0     2  copycat muslim terrorist arrested with assault...\n",
            "1     2  wow! chicago protester caught on camera admits...\n",
            "2     2   germany's fdp look to fill schaeuble's big shoes\n",
            "3     2  mi school sends welcome back packet warning ki...\n",
            "4     2  u.n. seeks 'massive' aid boost amid rohingya '...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values in training data:\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "print(\"\\nMissing values in testing data:\")\n",
        "print(test_df.isnull().sum())\n",
        "\n",
        "# Check for duplicates\n",
        "print(\"\\nNumber of duplicate rows in training data:\", train_df.duplicated().sum())\n",
        "print(\"Number of duplicate rows in testing data:\", test_df.duplicated().sum())\n",
        "\n",
        "# Display column names\n",
        "print(\"\\nTraining Data Columns:\", train_df.columns)\n",
        "print(\"Testing Data Columns:\", test_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASs2Tr18vQb0",
        "outputId": "3a69359e-3787-4bdb-9113-b1c9acc9a9d8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in training data:\n",
            "label    0\n",
            "text     0\n",
            "dtype: int64\n",
            "\n",
            "Missing values in testing data:\n",
            "label    0\n",
            "text     0\n",
            "dtype: int64\n",
            "\n",
            "Number of duplicate rows in training data: 1946\n",
            "Number of duplicate rows in testing data: 775\n",
            "\n",
            "Training Data Columns: Index(['label', 'text'], dtype='object')\n",
            "Testing Data Columns: Index(['label', 'text'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate rows\n",
        "train_df = train_df.drop_duplicates()\n",
        "test_df = test_df.drop_duplicates()\n",
        "\n",
        "# Drop missing values\n",
        "train_df = train_df.dropna()\n",
        "test_df = test_df.dropna()\n",
        "\n",
        "# Ensure labels are integers\n",
        "train_df['label'] = train_df['label'].astype(int)\n"
      ],
      "metadata": {
        "id": "Lu6oiaG4zZh4"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Text Processing Functions\n"
      ],
      "metadata": {
        "id": "hiXSVOe93HIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    \"\"\"Remove stopwords from a list of tokens.\"\"\"\n",
        "    return [word for word in tokens if word not in stopwords.words('english')]\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    \"\"\"Apply lemmatization to a list of tokens.\"\"\"\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and preprocess text by removing special characters, numbers, stopwords, and applying lemmatization.\"\"\"\n",
        "    text = str(text)  # Ensure it's a string\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\d+', ' ', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    tokens = remove_stopwords(tokens)  # Remove stopwords\n",
        "    tokens = lemmatize_tokens(tokens)  # Apply lemmatization\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "V5dDi1URtSsq"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Apply Text Cleaning\n",
        "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n",
        "test_df['cleaned_text'] = test_df['text'].apply(clean_text)\n",
        "\n",
        "# Step 6: Save Cleaned Data\n",
        "train_df.to_csv(\"/content/training_data_cleaned.csv\", index=False)\n",
        "test_df.to_csv(\"/content/testing_data_cleaned.csv\", index=False)"
      ],
      "metadata": {
        "id": "BCasCJkxzl0T"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure labels are integers\n",
        "train_df['label'] = train_df['label'].astype(int)\n",
        "\n",
        "# Check class distribution\n",
        "print(train_df['label'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpcXP75M4z8S",
        "outputId": "78f5ce5b-6220-47e1-b929-c85c45506d36"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "1    16181\n",
            "0    16025\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Convert Text to Numerical Representation (TF-IDF)"
      ],
      "metadata": {
        "id": "Na0loZod5Vmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = tfidf_vectorizer.fit_transform(train_df['cleaned_text'])\n",
        "y = train_df['label']\n",
        "\n",
        "\n",
        "#Split Data into Training and Validation Sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "ddEo5LAS5U5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "1WcWoW6376-I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NRO--a6t76w6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}